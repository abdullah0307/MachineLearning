{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oIgA8qyipR91"
   },
   "source": [
    "# AI Fundamentals - Assignment\n",
    "\n",
    "This assignment requires you to use [Tensorflow](https://www.tensorflow.org) and [Keras](https://keras.io/). Keras is a high-level Deep Learning API written in Python working as an interface to TensorFlow.\n",
    "\n",
    "This assignment is divided in two parts. In the first part you will learn about Keras with the help of the example below and the Keras [documentation](https://keras.io/). In the second part, you will practise training a Deep Learning model.\n",
    "\n",
    "## How to submit\n",
    "Submit by uploading this notebook to Canvas. It should include **plots**, **results** and **code** showing how the results were genereated.  [link text](https://)\n",
    "\n",
    "## Installation\n",
    "Instructions can be found here:\n",
    "* [Tensorflow](https://www.tensorflow.org/install/)\n",
    "\n",
    "Since Tensorflow 2.0, Keras is included in Tensorflow and will be automatically installed with Tensorflow. It can be accessed as ```tensorflow.keras```\n",
    "\n",
    "I recommend using ```pip```. For Tensorflow is it sufficient to install the CPU version. The GPU version requires a good workstation with high-end Nvidia GPU(s), and it is not necessary for this tutorial.\n",
    "\n",
    "If you're using a virtualenv:\n",
    "```\n",
    "pip3 install tensorflow\n",
    "```\n",
    "Add ```sudo``` for a systemwide installation (i.e. no ```virtualenv```).\n",
    "```\n",
    "sudo pip3 install tensorflow\n",
    "```\n",
    "Make sure that you have ```sklearn```, ```matplotlib``` and ```numpy``` installed, too.\n",
    "\n",
    "### Google Colab\n",
    "Tensorflow, Sklearn, Matplotlib and Numpy are already installed on Colab.\n",
    "\n",
    "\n",
    "## Part 1 - understand a model\n",
    "\n",
    "### Optimizers\n",
    "\n",
    "Loss is the penalty for a bad prediction. That is, loss is a number indicating how bad the model's prediction was on a single example. If the model's prediction is perfect, the loss is zero; otherwise, the loss is greater than zero. The goal of training a model is to find a set of weights and biases (i.e. parameters) that have, on average, a low loss across all examples. The term cost is used interchangably with loss. See the [loss section](https://keras.io/losses/) in the Keras documentation for a list and descriptions of what is available.\n",
    "\n",
    "![Side by side loss](https://drive.google.com/uc?id=1DdbQEQLCLCSw4uPsuf0C1nJCfUICT0Ae)\n",
    "<b>Figure 1.</b> Left: high loss and right: low loss.\n",
    "\n",
    "<!-- https://drive.google.com/file/d/1DdbQEQLCLCSw4uPsuf0C1nJCfUICT0Ae/view?usp=sharing\n",
    "<img src=\"./fig/LossSideBySide.png\" width=\"500\">\n",
    "<figcaption>Figure. Left: high loss and right: low loss.</figcaption>\n",
    " -->\n",
    "The optimizer is the algorithm used to minimize the loss/cost. Optimizers in neural networks work by finding the gradient/derivative of the loss with respect to the parameters (i.e. the weights). \"Gradient\" is the correct term since a we are looking at multi-dimensional systems (i.e. many parameters), however, the terms are often used interchangably. For those who didn't take multivariate calculus, just think of the gradient as a derivative. The derivative of the loss with respect to a parameters tells us how much the loss changes when we nudge a weight up or down. So, by knowing how a given parameter affects the loss the optimizer can change it so as to decrease the loss. The various optimizers differ in how they change the weights. \n",
    "\n",
    "#### Mini-overview over popular optimizers\n",
    "\n",
    "* **Stochastic Gradient Descent (SGD)**. This is the most basic and easy to understand optimizer. It updates the weights in the negative direction of the gradient by taking the average gradient of mini-batch of data (e.g. 20-1000 examples) in each step. Vanilla SGD only has one hyper-parameter, the learning rate. However, Kera's `SGD()` takes a few more arguments.\n",
    "* **Momentum**. This optimizer \"gains speed\" when the gradient has pointed in the same direction for several consecutive updates. That is, it has a momentum and want to keep moving in that direction. It gains momentum by accumulating an exponentially decaying moving average of past gradients. The step size depends on how large and aligned the sequence of gradients are. The most important hyper-parameter is alpha and common values are 0.5 and 0.9.\n",
    "* **Nesterov Momentum**. This is a modification of the standard momentum optimizer.\n",
    "* **AdaGrad**. This optimizer Ada-ptively sets the learning rate depending on the steepness/magnitude of the gradients. This is done so that weights with big gradients get a smaller effective learning rate, and weights with small gradients will get a greater effective learning rate. The result is quicker progress in the more gently sloped directions of the weight space and a slowdown in stepp regions.\n",
    "* **RMSProp**. This is modification of AdaGrad, where the accumulated gradient decays, that is, the influence of previous gradients gradually decreases.\n",
    "* **Adam**. The name comes from \"adaptive moments\", and it is a combination of RMSProp and momentum. It has several hyper-parameters.\n",
    "\n",
    "The above list just gives a quick overview of some of the most common. However, old optimizers are constantly improved and new are developed. SGD and momentum are most basic and easiest to understand and implement. They are still in use, but the more advanced optimizers tend to be better for practical use. Which one to use is generally an emperical question depending on both the data and the model.\n",
    "\n",
    "For a more complete overview of optimization algorithms see [this comparison](http://ruder.io/optimizing-gradient-descent/), and to see what is available in Keras, see the [optimizer section](https://keras.io/optimizers/) of the documentation.\n",
    "\n",
    "See the images below for a comparison of optimizers in a 2D space (NAG: Nesterov accelerated gradient, Adadelta: an extension of AdaGrad).\n",
    "\n",
    "![Contours - optimizer comparison](https://drive.google.com/uc?id=1CmrD-UPZ7EIUjRuO_ib7k9CL1FO2bbLk)\n",
    "<b>Figure 2.</b> Comparison of six different optimizers.\n",
    "\n",
    "\n",
    "![Saddle point - optimizer comparison](https://drive.google.com/uc?id=1QVhN9rAvCjXtGyNZkmFivyyCzNsntObh)\n",
    "<b>Figure 3.</b> Comparison of six different optimizers at a saddle point.\n",
    "\n",
    "<!-- <img src=\"./fig/contours_evaluation_optimizers.gif\" width=\"500\">\n",
    "<img src=\"./fig/saddle_point_evaluation_optimizers.gif\" width=\"500\"> -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 371
    },
    "id": "45_y3L73pR93",
    "outputId": "0c101b5c-dbd5-45ea-8157-0a76bc57c9b1"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-bf2f9dce0f26>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfetch_openml\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mnist_784'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mversion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_X_y\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;31m# Normalize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m255.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5139\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5140\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5141\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5143\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'reshape'"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "# for the random seed\n",
    "import tensorflow as tf\n",
    "\n",
    "# set the random seeds to get reproducible results\n",
    "np.random.seed(1)\n",
    "tf.random.set_seed(2)\n",
    "\n",
    "# Load data from https://www.openml.org/d/554\n",
    "X, y = fetch_openml('mnist_784', version=1, return_X_y=True)\n",
    "X, y = X[:1000], y[:1000]\n",
    "X = X.reshape(X.shape[0], 28, 28, 1)\n",
    "# Normalize\n",
    "X = X / 255.\n",
    "# number of unique classes\n",
    "num_classes = len(np.unique(y))\n",
    "y = y.astype(int)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=1)\n",
    "\n",
    "num_tot = y.shape[0]\n",
    "num_train = y_train.shape[0]\n",
    "num_test = y_test.shape[0]\n",
    "\n",
    "y_oh = np.zeros((num_tot, num_classes))\n",
    "y_oh[range(num_tot), y] = 1\n",
    "\n",
    "y_oh_train = np.zeros((num_train, num_classes))\n",
    "y_oh_train[range(num_train), y_train] = 1\n",
    "\n",
    "y_oh_test = np.zeros((num_test, num_classes))\n",
    "y_oh_test[range(num_test), y_test] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qodild4KpR94"
   },
   "source": [
    "### Question 1\n",
    "**The data set**\n",
    "\n",
    "Plot a three examples from the data set.\n",
    "* What type of data are in the data set?\n",
    "\n",
    "    <span style=\"color:red\"> <*answer here*> </span>\n",
    "    \n",
    "\n",
    "* What does the line ```X = X.reshape(X.shape[0], 28, 28, 1)``` do?\n",
    "\n",
    "Look at how the encoding of the targets (i.e. ```y```) is changed. E.g. the lines\n",
    "```\n",
    "    y_oh = np.zeros((num_tot, num_classes))\n",
    "    y_oh[range(num_tot), y] = 1\n",
    "```\n",
    "Print out a few rows of ```y``` next to ```y_oh```.\n",
    "* What is the relationship between ```y``` and ```y_oh```?\n",
    "\n",
    "    <span style=\"color:red\"> <*answer here*> </span>\n",
    "    \n",
    "    \n",
    "* What is the type of encoding in ```y_oh``` called and why is it used?\n",
    "\n",
    "    <span style=\"color:red\"> <*answer here*> </span>\n",
    "    \n",
    "    \n",
    "* Plot three data examples in the same figure and set the correct label as title. \n",
    "    * It should be possible to see what the data represent.\n",
    "        \n",
    "    <span style=\"color:red\"> <*Plot in the cell below*> </span>        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SVsVYQGepR94"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TCYB2nWxpR94"
   },
   "source": [
    "### Question 2\n",
    "**The model**\n",
    "\n",
    "Below is some code for bulding and training a model with Keras.\n",
    "* What type of network is implemented below? I.e. a normal MLP, RNN, CNN, Logistic Regression...?\n",
    "\n",
    "    <span style=\"color:red\"> <*answer here*> </span>\n",
    "    \n",
    "    \n",
    "* What does ```Dropout()``` do?\n",
    "\n",
    "    <span style=\"color:red\"> <*answer here*> </span>\n",
    "\n",
    "\n",
    "* Which type of activation function is used for the hidden layers?\n",
    "\n",
    "    <span style=\"color:red\"> <*answer here*> </span>\n",
    "\n",
    "\n",
    "* Which type of activation function is used for the output layer?\n",
    "\n",
    "    <span style=\"color:red\"> <*answer here*> </span>\n",
    "\n",
    "\n",
    "* Why are two different activation functions used?\n",
    "\n",
    "    <span style=\"color:red\"> <*answer here*> </span>\n",
    "\n",
    "\n",
    "* What optimizer is used in the model below?\n",
    "\n",
    "    <span style=\"color:red\"> <*answer here*> </span>\n",
    "\n",
    "\n",
    "* How often are the weights updated (i.e. after how many data examples)?\n",
    "\n",
    "    <span style=\"color:red\"> <*answer here*> </span>\n",
    "\n",
    "\n",
    "* What loss function is used?\n",
    "\n",
    "    <span style=\"color:red\"> <*answer here*> </span>\n",
    "\n",
    "\n",
    "* How many parameters (i.e. weights and biases, NOT hyper-parameters) does the model have?\n",
    "\n",
    "    <span style=\"color:red\"> <*answer here*> </span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pQlAL1ikpR94",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(16, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "# Max pooling\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.))\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "# Max pooling\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.))\n",
    "\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_oh_train, batch_size=32, epochs=60)\n",
    "\n",
    "# Evaluate performance\n",
    "test_loss = model.evaluate(X_test, y_oh_test, batch_size=32)\n",
    "\n",
    "predictions = model.predict(X_test, batch_size=32)\n",
    "predictions = np.argmax(predictions, axis=1) # change encoding again\n",
    "print('Accuracy:', (predictions == y_test).sum() / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MZAOAlRNpR94"
   },
   "source": [
    "## Part 2 - train a model\n",
    "\n",
    "A model's performance depends on many factors apart from the model architecture (e.g. type and number of layers) and the dataset. Here you will get to explore some of the factors that affect model performance. Much of the skill in training deep learning models lies in quickly finding good values/options for these choises.\n",
    "\n",
    "In order to observe the learning process it is best to compare the training set loss with the loss on the test set. How to visualize these variables with Keras is described under [Training history visualization](https://keras.io/visualization/#training-history-visualization) in the documentation.\n",
    "\n",
    "You will explore the effect of 1) optimizer, 2) training duration, and 3) dropout (see the question above).\n",
    "\n",
    "When training, an **epoch** is one pass through the full training set.\n",
    "\n",
    "### Question 3\n",
    "\n",
    "* **Vizualize the training**. Use the model above to observe the training process. Train it for 150 epochs and then plot both `loss` and `val_loss` (i.e. loss on the valiadtion set, here the terms \"validation set\" and \"test set\" are used interchangably, but this is not always true). What is the optimal number of epochs for minimizing the test set loss? \n",
    "    * Remember to first reset the weights (this can be done by calling ```model.compile()```), otherwise the training just continues from where it was stopped earlier.\n",
    "\n",
    "* **Optimizer**. Select three different optimizers and for each find the close-to-optimal hyper-parameter(s). In your answer, include a) your three choises, b) best hyper-parameters for each of the three optimizers and, c) the code that produced the results.\n",
    "    * *NOTE* that how long the training takes varies with optimizer. I.e., make sure that the model is trained for long enough to reach optimal performance.\n",
    "\n",
    "* **Dropout**. Use the best optimizer and do hyper-parameter seach and find the best value for ```Dropout()```.\n",
    "\n",
    "* **Best model**. Combine the what you learned from the above three questions to build the best model. How much better is it than the worst and average models?\n",
    "\n",
    "    <span style=\"color:red\"> <*answer here*> </span>\n",
    "\n",
    "\n",
    "* **Results on the test set**. When doing this search for good model configuration/hyper-parameter values, the data set was split into *two* parts: a training set and a test set (the term \"validation\" was used interchangably with \"test\"). For your final model, is the performance (i.e. accuracy) on the test set representative for the performance one would expect on a previously unseen data set (drawn from the same distribution)? Why?\n",
    "\n",
    "    <span style=\"color:red\"> <*answer here*> </span>\n",
    "\n",
    "\n",
    "## Further information\n",
    "For ideas about hyper-parameter tuning, take a look at the strategies described in the sklearn documentation under [model selection](https://scikit-learn.org/stable/model_selection.html), or in this [blog post](https://blog.tensorflow.org/2020/01/hyperparameter-tuning-with-keras-tuner.html) from TensorFlow. For a more thorough discussion about optimizers see [this video](https://www.youtube.com/watch?v=DiNzQP7kK-s) discussing the article [Descending through a Crowded Valley -- Benchmarking Deep Learning Optimizers](https://arxiv.org/abs/2007.01547).\n",
    "\n",
    "\n",
    "**Good luck!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MpvmZOScpR95"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "MMAI5000_assignment3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
